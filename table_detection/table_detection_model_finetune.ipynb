{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TableTransformerForObjectDetection, TableTransformerConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import DetrImageProcessor\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd \n",
    "\n",
    "dat = str(date.today())\n",
    "# dat\n",
    "dt = datetime.today().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "dt\n",
    "\n",
    "\n",
    "#work/Sagar/tabel-transformer-model/config.json\n",
    "\n",
    "finetune_count = 1\n",
    "\n",
    "# /Users/sagar17.patil/Documents/work_dir/ocr/layoutlmv3_playgorund\n",
    "data_folder_path = \"/home/jovyan/work/Sagar/p2p_august_5_to_11_data\"\n",
    "current_working_directory_path = \"/home/jovyan/work/Sagar/table-transformer-playground\"\n",
    "other_supporting_data_path = \"/home/jovyan/work/Sagar/p2p_august_5_to_11_data/other_supporting_data\"\n",
    "other_supporting_data_table_tansformer_path = \"/home/jovyan/work/Sagar/p2p_august_5_to_11_data/other_supporting_data/tbl-trsmf-data\"\n",
    "\n",
    "finetuned_model_path = os.path.join(current_working_directory_path, f\"finetuned_models/{dat}/{finetune_count}\")\n",
    "os.makedirs(finetuned_model_path, exist_ok=True)\n",
    "\n",
    "dates = ['2024-08-05','2024-08-06','2024-08-07','2024-08-08','2024-08-09','2024-08-10','2024-08-11']\n",
    "data_date_range = \"5_to_11_august\"\n",
    "\n",
    "# os.environ['HF_DATASETS_OFFLINE'] = \"1\"\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = \"1\"\n",
    "\n",
    "# Load a pre-trained Table Transformer model and processor\n",
    "model_name = \"/home/jovyan/work/Sagar/table_transformer_detection_model\"\n",
    "# model_name = \"microsoft/table-transformer-detection\"\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(model_name)\n",
    "model = TableTransformerForObjectDetection.from_pretrained(pretrained_model_name_or_path = model_name,use_pretrained_backbone=False ,local_files_only=True,cache_dir=model_name)\n",
    "# model = TableTransformerForObjectDetection.from_pretrained(model_name,cache_dir=model_name)\n",
    "\n",
    "# model = torch.load(model_name)\n",
    "# model.eval()\n",
    "\n",
    "dataset_type = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_page_files_tables_path = os.path.join(other_supporting_data_table_tansformer_path, f\"single_page_files_tables_list_{dataset_type}_3_sd.json\")\n",
    "#Write dictionary to JSON file\n",
    "with open (single_page_files_tables_path, 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "data_list1 = data_list.copy()\n",
    "len_data_list = len(data_list1)\n",
    "print(len_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDetectionDataset(Dataset):\n",
    "    def __init__(self, data, processor):\n",
    "        self.dataset = data\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _get_single_item(self, idx):\n",
    "        # Load image\n",
    "        fname = self.dataset[idx][\"page_file_name\"] + \".png\"\n",
    "        image_path = os.path.join(data_folder_path, os.path.join(os.path.join(self.dataset[idx][\"date\"], \"image_seperated_pdfs\"), fname))\n",
    "        \n",
    "        # print(fname)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        width_px, height_px = image.size\n",
    "        dpi = 300\n",
    "\n",
    "        tables_bboxes = self.dataset[idx][\"tables_bboxes\"].copy()\n",
    "\n",
    "        def create_annotations(tables_bboxes, width_px, height_px, dpi):\n",
    "            annotations = []\n",
    "            class_labels = []\n",
    "            boxes = []\n",
    "\n",
    "            if fname[:7] != \"sample_\":\n",
    "                # print(\"tables_bboxes: \",len(tables_bboxes))\n",
    "\n",
    "                for bb in tables_bboxes:\n",
    "                    # Convert coordinates to pixels and normalize\n",
    "                    x1 = (bb[0] * dpi) / width_px\n",
    "                    y1 = (bb[1] * dpi) / height_px\n",
    "                    x2 = (bb[4] * dpi) / width_px\n",
    "                    y2 = (bb[5] * dpi) / height_px\n",
    "                    \n",
    "                    # print([x1, y1, x2, y2])\n",
    "\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    class_labels.append(1)  # 1 for table class\n",
    "                # print(boxes)\n",
    "            else:\n",
    "                # print(\"tables_bboxes: \",tables_bboxes)\n",
    "                # print(width_px, height_px)\n",
    "                for bb in tables_bboxes:\n",
    "                    # Convert coordinates to pixels and normalize\n",
    "                    x1 = (bb[0]) / width_px\n",
    "                    y1 = (bb[1]) / height_px\n",
    "                    x2 = (bb[2]) / width_px\n",
    "                    y2 = (bb[3]) / height_px\n",
    "\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    class_labels.append(1)  # 1 for table class\n",
    "\n",
    "\n",
    "            return torch.tensor(class_labels, dtype=torch.long), torch.tensor(boxes, dtype=torch.float)\n",
    "\n",
    "        # Get class labels and boxes\n",
    "        class_labels, boxes = create_annotations(tables_bboxes, width_px, height_px, dpi)\n",
    "\n",
    "        # Process the image\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            do_resize=True,\n",
    "            size={\"height\": 800, \"width\": 800},\n",
    "            resample=Image.Resampling.BILINEAR,\n",
    "            do_rescale=True,\n",
    "            do_normalize=True,\n",
    "            do_pad=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # Remove batch dimension\n",
    "\n",
    "        return pixel_values, {\n",
    "            \"class_labels\": class_labels,\n",
    "            \"boxes\": boxes\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._get_single_item(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_list, processor):\n",
    "    main_list = []\n",
    "    failed_list = []\n",
    "    ds = TableDetectionDataset(data_list, processor)\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        try:\n",
    "            \n",
    "            # dataset = TableDetectionDataset([data_list[i]], processor)\n",
    "            item = ds.__getitem__(i)\n",
    "            main_list.append(item)\n",
    "            # print(\"successful: \",i)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on index {i}:\", e)\n",
    "            failed_list.append(i)\n",
    "            continue\n",
    "    \n",
    "    return main_list\n",
    "\n",
    "# Usage\n",
    "dataset = prepare_data(data_list1, processor)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# dataset = prepare_data(data_list1, processor)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[0] for item in batch])\n",
    "    targets = [{\n",
    "        \"class_labels\": item[1][\"class_labels\"],\n",
    "        \"boxes\": item[1][\"boxes\"]\n",
    "    } for item in batch]\n",
    "    return pixel_values, targets\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs, device):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    ep_cnt = 0\n",
    "    \n",
    "    loss_metrics = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        loss1 = 0\n",
    "        \n",
    "        # print(f\"\\n\\t Current Epoch: {epoch+1} - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "        \n",
    "        for batch_idx, (pixel_values, targets) in enumerate(dataloader):\n",
    "            try:\n",
    "                print(f\"\\n\\t - - - - - - - batch {batch_idx} - - - - - - -\")\n",
    "                # Move inputs to device\n",
    "                pixel_values = pixel_values.to(device)\n",
    "                targets = [{\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                } for t in targets]\n",
    "                \n",
    "                print(\"1\")\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(pixel_values=pixel_values, labels=targets)\n",
    "                \n",
    "                print(\"2\")\n",
    "                loss = outputs.loss\n",
    "                print(\"3\")\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                print(\"4\")\n",
    "                # loss1 = loss.item()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # if batch_idx % 10 == 0:\n",
    "                #     print(f\"\\t Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}:\", e)\n",
    "                print(len(pixel_values), len(targets))\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"\\n Epoch {epoch+1} completed, Average Loss: {avg_loss:.4f}\")\n",
    "        d = {\n",
    "            \"epoch\":epoch,\n",
    "            \"avg_loss\":avg_loss\n",
    "        }\n",
    "        loss_metrics.append(d)\n",
    "        \n",
    "        ep_cnt = epoch+1\n",
    "        \n",
    "        if ep_cnt in [3,5,8,10]:\n",
    "\n",
    "            model_state_dict_path = os.path.join(finetuned_model_path, f'ttft_{dt}_total_epoch_{num_epochs}_bs_{batch_size}_epcnt_{ep_cnt}.pth')\n",
    "\n",
    "            torch.save(model.state_dict(), model_state_dict_path)\n",
    "            print(\"model saved on path:\",model_state_dict_path)\n",
    "            \n",
    "    model_state_dict_path = os.path.join(finetuned_model_path, f'ttft_{dt}_total_epoch_{num_epochs}_bs_{batch_size}_epcnt_{ep_cnt}.pth')\n",
    "\n",
    "    torch.save(model.state_dict(), model_state_dict_path)\n",
    "    print(\"model saved on path:\",model_state_dict_path)\n",
    "    \n",
    "    return loss_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 16\n",
    "\n",
    "loss_metrics = train_model(model, dataloader, num_epochs=num_epochs, device=device)\n",
    "\n",
    "model_state_dict_path = os.path.join(finetuned_model_path, f'training_loss_metrics_{dt}_tot_epoch_{num_epochs}_bs_{batch_size}.json')\n",
    "with open(model_state_dict_path, \"w\") as f:\n",
    "    json.dump(loss_metrics, f, indent=3)  # indent=4 makes it pretty-printed\n",
    "    f.close()\n",
    "    \n",
    "print(\"loss metrics file saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import TableTransformerForObjectDetection, TableTransformerConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import DetrImageProcessor\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "\n",
    "\n",
    "# /Users/sagar17.patil/Documents/work_dir/ocr/layoutlmv3_playgorund\n",
    "data_folder_path = \"/home/jovyan/work/Sagar/p2p_august_5_to_11_data\"\n",
    "current_working_directory_path = \"/home/jovyan/work/Sagar/table-transformer-playground\"\n",
    "other_supporting_data_path = \"/home/jovyan/work/Sagar/p2p_august_5_to_11_data/other_supporting_data\"\n",
    "other_supporting_data_table_tansformer_path = \"/home/jovyan/work/Sagar/p2p_august_5_to_11_data/other_supporting_data/tbl-trsmf-data\"\n",
    "\n",
    "dates = ['2024-08-05','2024-08-06','2024-08-07','2024-08-08','2024-08-09','2024-08-10','2024-08-11']\n",
    "data_date_range = \"5_to_11_august\"\n",
    "\n",
    "#dates of which data is to be prepared\n",
    "dates = ['2024-08-05','2024-08-06','2024-08-07','2024-08-08','2024-08-09','2024-08-10','2024-08-11']\n",
    "data_date_range = \"5_to_11_august\"\n",
    "# dates = ['2024-08-05']\n",
    "\n",
    "# os.environ['HF_DATASETS_OFFLINE'] = \"1\"\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = \"1\"\n",
    "\n",
    "dataset_type = \"test\"\n",
    "\n",
    "single_page_files_tables_path = os.path.join(other_supporting_data_table_tansformer_path, f\"single_page_files_tables_list_{dataset_type}_1.json\")\n",
    "\n",
    "#Write dictionary to JSON file\n",
    "with open (single_page_files_tables_path, 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "\n",
    "len_data_list = len(data_list)\n",
    "print(len_data_list)\n",
    "\n",
    "ele1 = data_list[0].copy()\n",
    "ele1.keys()\n",
    "\n",
    "\n",
    "model_name = \"/home/jovyan/work/Sagar/table_transformer_detection_model\"\n",
    "state_dict_path = \"/home/jovyan/work/Sagar/table-transformer-playground/ttft_2025-02-06_05:43:12_epoch_10_bs_16.pth\"\n",
    "# Initialize model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# config = TableTransformerConfig.from_pretrained(model_name)\n",
    "print(\"1\")\n",
    "model = TableTransformerForObjectDetection.from_pretrained(pretrained_model_name_or_path = model_name,use_pretrained_backbone=False ,local_files_only=True,cache_dir=model_name)\n",
    "\n",
    "\n",
    "print(\"2\")\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "import torch\n",
    "from transformers import TableTransformerForObjectDetection, TableTransformerConfig\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class TableDetector:\n",
    "    def __init__(self, model_path=None, confidence_threshold=0.7):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "#         model_name = \"/home/jovyan/work/Sagar/table_transformer_detection_model\"\n",
    "#         state_dict_path = \"/home/jovyan/work/Sagar/table-transformer-playground/ttft_2025-02-06_05:43:12_epoch_10_bs_16.pth\"\n",
    "#         # Initialize model\n",
    "        \n",
    "#         config = TableTransformerConfig.from_pretrained(model_name)\n",
    "#         print(\"1\")\n",
    "#         self.model = TableTransformerForObjectDetection.from_pretrained(pretrained_model_name_or_path = model_name,use_pretrained_backbone=False ,local_files_only=True,cache_dir=model_name)\n",
    "#         print(\"2\")\n",
    "#         state_dict = torch.load(state_dict_path, map_location=self.device)\n",
    "#         self.model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "            \n",
    "#         self.model.to(self.device)\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        processor = DetrImageProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((800, 800)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def _fix_box_coordinates(self, box):\n",
    "        \"\"\"\n",
    "        Ensure box coordinates are in the correct order (x1 <= x2 and y1 <= y2)\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = box\n",
    "        return [\n",
    "            min(x1, x2),  # x1\n",
    "            min(y1, y2),  # y1\n",
    "            max(x1, x2),  # x2\n",
    "            max(y1, y2)   # y2\n",
    "        ]\n",
    "\n",
    "    def _validate_box(self, box, image_size):\n",
    "        \"\"\"\n",
    "        Validate and clip box coordinates to image boundaries\n",
    "        \"\"\"\n",
    "        width, height = image_size\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        x1 = max(0, min(x1, width))\n",
    "        y1 = max(0, min(y1, height))\n",
    "        x2 = max(0, min(x2, width))\n",
    "        y2 = max(0, min(y2, height))\n",
    "        \n",
    "        # Ensure minimum box size\n",
    "        if x2 - x1 < 1 or y2 - y1 < 1:\n",
    "            return None\n",
    "            \n",
    "        return [x1, y1, x2, y2]\n",
    "\n",
    "    def detect_tables(self, image_path):\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        original_size = image.size\n",
    "        \n",
    "        # Transform image\n",
    "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(img_tensor)\n",
    "        \n",
    "        # Process outputs\n",
    "        probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "        keep = probas.max(-1).values > self.confidence_threshold\n",
    "        \n",
    "        # Convert boxes to original image size\n",
    "        boxes = outputs.pred_boxes[0, keep].cpu()\n",
    "        scores = probas[keep].cpu()\n",
    "        \n",
    "        # Denormalize boxes to original image size\n",
    "        boxes = self._rescale_boxes(boxes, original_size)\n",
    "        \n",
    "        # Fix and validate boxes\n",
    "        valid_boxes = []\n",
    "        valid_scores = []\n",
    "        for box, score in zip(boxes, scores):\n",
    "            box = self._fix_box_coordinates(box)\n",
    "            box = self._validate_box(box, original_size)\n",
    "            if box is not None:\n",
    "                valid_boxes.append(box)\n",
    "                valid_scores.append(score)\n",
    "        \n",
    "        return np.array(valid_boxes), np.array(valid_scores)\n",
    "\n",
    "    def _rescale_boxes(self, boxes, original_size):\n",
    "        width, height = original_size\n",
    "        boxes = boxes * torch.tensor([width, height, width, height], dtype=torch.float32)\n",
    "        return boxes\n",
    "\n",
    "    def visualize_detections(self, image_path, output_path, boxes, scores):\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # Detect tables\n",
    "        # boxes, scores = self.detect_tables(image_path)\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            print(\"No tables detected!\")\n",
    "            image.save(output_path)\n",
    "            return image\n",
    "        \n",
    "        # Draw boxes\n",
    "        for box, score in zip(boxes, scores):\n",
    "            if score > 0.40:\n",
    "                print(score)\n",
    "                box = box.astype(np.int32)\n",
    "                try:\n",
    "                    # Draw rectangle\n",
    "                    draw.rectangle(\n",
    "                        [(box[0], box[1]), (box[2], box[3])],\n",
    "                        outline='red',\n",
    "                        width=4\n",
    "                    )\n",
    "\n",
    "                    # Add confidence score\n",
    "                    text = f'Table: {score.max():.2f}'\n",
    "                    text_position = (int(box[0]), max(0, int(box[1] - 20)))\n",
    "                    draw.text(\n",
    "                        text_position,\n",
    "                        text,\n",
    "                        fill='red'\n",
    "\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to draw box {box}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Save result\n",
    "        \n",
    "        image.save(output_path)\n",
    "        # print(\"image saved on path: \",output_path)\n",
    "        return output_path\n",
    "\n",
    "tbls = []\n",
    "def make_predictions(ele1=None):\n",
    "    # Initialize detector\n",
    "    detector = TableDetector(\n",
    "        model_path=None,  # Use pretrained model\n",
    "        confidence_threshold=0.0\n",
    "    )\n",
    "\n",
    "#     width_in = ele1[\"width\"]\n",
    "#     height_in = ele1[\"height\"]\n",
    "#     dpi = 300\n",
    "#     polygon_in = ele1[\"tables\"][0][\"boundingRegions\"][0][\"polygon\"]\n",
    "    \n",
    "#     fn1 = str(ele1[\"page_file_name\"])\n",
    "    \n",
    "#     fname = ele1[\"page_file_name\"]+\".png\"\n",
    "    \n",
    "#     output_path = os.path.join(current_working_directory_path,f\"original/{fname}\")\n",
    "    \n",
    "#     image_path = os.path.join(data_folder_path,os.path.join(os.path.join(ele1[\"date\"], \"image_seperated_pdfs\"),fname))\n",
    "\n",
    "\n",
    "    width_in = 2480\n",
    "    height_in = 3508\n",
    "    dpi = 300\n",
    "    # polygon_in = ele1[\"tables\"][0][\"boundingRegions\"][0][\"polygon\"]\n",
    "    \n",
    "#     fn1 = str(ele1[\"page_file_name\"])\n",
    "    \n",
    "#     fname = ele1[\"page_file_name\"]+\".png\"\n",
    "    \n",
    "#     output_path = os.path.join(current_working_directory_path,f\"original/{fname}\")\n",
    "    \n",
    "#     image_path = os.path.join(data_folder_path,os.path.join(os.path.join(ele1[\"date\"], \"image_seperated_pdfs\"),fname))\n",
    "\n",
    "    \n",
    "    # Example usage\n",
    "    image_path = './a4_table_with_text.png'\n",
    "    output_path = './a4_table_with_text_1.png'\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Get detection coordinates and scores\n",
    "        boxes, scores = detector.detect_tables(image_path)\n",
    "        print(\"Detected Tables:\")\n",
    "        \n",
    "        for i, (box, score) in enumerate(zip(boxes, scores)):\n",
    "            d = {}\n",
    "            print(f\"Table {i+1}:\")\n",
    "            print(f\"  Coordinates: {box}\")\n",
    "            print(f\"  Confidence: {score.max():.2f}\")\n",
    "            \n",
    "        \n",
    "        # Method 2: Visualize detections\n",
    "        saved_path = detector.visualize_detections(image_path, output_path, boxes, scores)\n",
    "        print(f\"Visualization saved to {saved_path}\")\n",
    "        d = {\n",
    "            \"bbox\":boxes,\n",
    "            \"confidence\":scores\n",
    "        }\n",
    "        tbls.append(d)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during detection: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    make_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
